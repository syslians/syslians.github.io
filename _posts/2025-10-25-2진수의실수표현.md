---
layout: post
title: "VirtualSwitch(VMware)"
date: 2025-07-27 22:00:00 +0900
categories: [ComputerScience]
author: cotes
published: true
---



## 고정소수점 vs 부동소수점: 컴퓨터 실수 표현의 두 가지 방식

컴퓨터에서 실수(real number)를 표현하는 방법은 정수 표현보다 훨씬 복잡합니다. 그 이유는 컴퓨터가 실수를 정수와 마찬가지로 2진수(0과 1)로만 표현해야 하기 때문입니다. 
이러한 한계를 극복하기 위해 고정소수점(fixed-point) 방식과 부동소수점(floating-point) 방식이라는 두 가지 대표적인 실수 표현 방법이 널리 활용됩니다. 
이번 글에서는 두 방식의 개념과 동작 원리를 Python 코드 예시와 함께 살펴보고, 장단점을 비교한 뒤 실무에서 유용한 팁을 정리해보겠습니다.

### 고정소수점: 개념 

고정소수점 방식은 말 그대로 소수점의 위치를 미리 고정해 두고 실수를 표현하는 방법입니다. 고정소수점에서는 정수부를 표현하는 비트와 소수부를 표현하는 비트의 개수를 미리 고정하여 그 비트들만을 사용해 실수를 표현합니다.
예를 들어 최상위 1비트를 부호(bit)로 사용하고, 다음 N비트를 정수 부분, 이후 M비트를 소수 부분으로 할당했다면, 소수점은 정수부와 소수부 사이에 고정되어 항상 해당 위치를 기준으로 실수가 저장됩니다.

![BitOperations](https://resources-public-blog.modulabs.co.kr/blog/prd/content/259164/fixed-points.png)
[그림1. 고정소수점 https://resources-public-blog.modulabs.co.kr/blog/prd/content/259164/fixed-points.png]

예를 들어 32비트 고정소수점에서 부호 1비트 + 정수부 15비트 + 소수부 16비트로 구조를 정했다고 가정해봅시다. 10진수 7.625를 이 방식으로 표현해 보면, 7.625(10)를 2진수로 변환한 값은 111.101(2)이 됩니다.
부호 비트는 양수이므로 0, 정수부 15비트에는 111 (7에 해당)을 넣고 나머지는 0으로 채웁니다. 소수부 16비트에는 .101 (0.625에 해당) 부분을 채운 후 남는 자리를 0으로 채웁니다. 그 결과 7.625의 32비트 이진 표현은 다음과 같습니다:

```
0 000000000000111 1010000000000000 (2)

```

(각 부분은 순서대로 부호 1비트, 정수부 15비트, 소수부 16비트)

위 비트열을 보면, 소수점이 부호/정수부와 소수부 경계에 고정되어 있음을 알 수 있습니다. 고정소수점 방식에서는 이렇게 미리 정해둔 자리수까지만 소수 부분을 저장하기 때문에, 정수 연산만으로 실수 계산을 흉내낼 수 있다는 장점이 있습니다. 
실제로 파이썬 등 고급 언어에서도 정수 연산을 활용해 고정소수점 계산을 구현할 수 있습니다. 예를 들어 소수점 둘째 자리까지 표현하는 100분의 1 스케일을 고정하여 금액을 더하는 코드는 다음과 같습니다:

```python
# 2(decimal) fixed-point arithmetic (e.g., currency in cents)
scale = 100  # 고정 소수점 스케일: 100 => 소수점 둘째 자리
price = 795   # 7.95달러를 센트로 표현 (정수 795)
tax = 57      # 0.57달러를 센트로 표현 (정수 57)
total = price + tax  # 정수 덧셈으로 합계 계산
print(f"총액: {total} 센트 (고정소수점) = {total/scale:.2f} 달러")
# 출력: 총액: 852 센트 (고정소수점) = 8.52 달러

```

위 예시처럼 고정소수점에서는 소수점을 움직이지 않고도 정수 연산만으로 실수 계산을 수행할 수 있기 때문에, 실수 표현과 연산 방법이 비교적 단순하고 속도가 빠르다는 장점이 있습니다.
그러나 고정된 비트 내에서만 정수부와 소수부를 나타내므로 표현할 수 있는 숫자의 범위가 제한적이고 정밀도 역시 낮은 편입니다.
정수부 비트를 늘리면 표현 가능한 최대 숫자는 커지지만 소수부가 줄어 정밀도가 떨어지고, 반대로 소수부 비트를 늘리면 정밀도는 높아지지만 표현 가능한 범위의 한계가 금방 드러납니다.
이러한 제약 때문에 범용 컴퓨터보다는 높은 정밀도가 필요 없거나 하드웨어 자원이 제한된 소규모 임베디드 시스템 등에서만 고정소수점이 간혹 활용되고, 일반적인 실수 계산이 필요한 범용 시스템에서는 잘 쓰이지 않습니다.
이 한계를 극복하기 위해 등장한 방식이 바로 다음에 설명할 부동소수점 방식입니다.

### 고정 소수점의 대표적 활용 사례
- **임베디드 시스템 및 DSP 분야**: 마이크로컨트롤러나 IOT 디바이스처럼 하드웨어 자원이 제한된 환경에서는 고정소수점 연산이 널리 사용됩니다.
  이러한 장치들은 하드웨어 부동소수점 연산장치(FPU)가 없거나 전력 소비를 줄이기 위해 정수 연산을 사용하며, 디지털 신호 처리장치(DSP) 업무(예: 센서 신호,
  오디오 코덱 등)에 고정소수점을 사용합니다. 실제로 MP3, Ogg Vorbis 같은 오디오 디코딩 라이브러리들은 많은 재생 기기에 FPU가 없다는 이유로
  고정소수점 산술을 채택했습니다. 고정소수점 DSP 칩은 대량 생산되는 소비자 전자제품에서 부동소수점 DSP보다 비용 효율적으로 쓰이며, 적은 전력과
  메모리로 실시간 처리를 가능하게 합니다.

- **금융 및 회계 소프트웨어**: 통화 계산과 같이 정확도가 필수적인 금융 분야에서는 부동소수점의 미세한 반올림 오류를 피하기 위해 고정소수점(또는 정수 기반)
  방식을 많이 씁니다. 예를 들어 원화/달러 금액을 부동소수점으로 다룰 경우 2.78 같은 값이 내부적으로 2.77999997… 로 저장되어 1원 미만의 오차를
  초래할 수 있습니다. 이러한 문제를 피하기 위해 Modern Treasury 같은 핀테크 기업에서는 금액의 센트(1/100 달러) 단위를 정수로 저장하여 소수점
  오류를 없앱니다. 고정된 소수 자릿수를 사용하는 Decimal 자료형 역시 금융권에서 널리 활용되며, 통화 단위를 정확히 표현해 부동소수점 오차로 인한
  센트 단위 손실을 방지합니다.

- **제한된 연산장치를 가진 하드웨어(게임 콘솔 등)**: 부동소수점 연산이 비용면에서 부담되던 시대의 하드웨어에서는 모든 실수 계산을 고정소수점으로 처리했습니다.
  대표적으로 1990년대 게임 콘솔(5세대)들은 FPU가 없어서 3D 게임 연산을 전부 고정소수점으로 수행했습니다. 예를 들어 플레이스테이션1 과 Sega Saturn
  등의 3D 게임기는 16.16 또는 32.16 형식의 고정소수점 연산을 통해 그래픽 변환을 처리했습니다. 그결과 PS1 게임에서 물체가 미세하게 떨리는 소위 "Wabble"
  현상이 나타나기도 했는데, 이는 고정소수점 정밀도 제한 때문이였습니다. 이처럼 하드웨어 부동소수점 지원이 없는 시스템(초기 휴대폰, 옛날 PC의 386,486SX 등)
  에서는 고정소수점이 사실상 표준으로 활용되었습니다.

### 고정 소수점의 주요 엣지 케이스 (한계 및 주의점)
- **범위 제한에 따른 Overflow/Underflow**: 고정소수점은 표현할 수 있는 수의 범위가 고정되어 있기 때문에, 연산 결과가 그 범위를 벗어나면 overflow가
  발생합니다. 예를 들어 8비트 고정소수점에서 최댓값이 127이라면, 127에 1을 더하는 연산 시 값이 순환(wrap-aroung)되어 -128로 바뀌거나(2의 보수 표현 기준)
  잘못된 결과를 낳을 수 있습니다. 일부 시스템은 이러한 경우 값을 최대치로 포화(saturation)시키기도 하지만, 어느 쪽이든 본래 수학적 결과와 달라집니다.
  Ariane5 로켓 폭발 사고는 이러한 overflow 문제의 극단적 사례인데, 64비트 부동소수점 값을 16비트 정수(고정 소수점)로 변환하는 과정에서 값이 범위를
  넘어 오버플로우가 발생했고, 잘못된 데이터로 인해 로켓이 자폭했습니다. 이처럼 고정소수점에서는 설계 시 예상한 최대/최소 범위를 벗어나는 값이 나오면 심각한
  오류로 이어질 수 있습니다. 반대로, 너무 작은 값은 정해진 소수부 비트로 표현하지 못해 0으로 언더플로우 될 수도 있습니다. 즉, 고정소수점은 사전에 정한 범위
  내에서만 정확하며, 그 밖의 표현 불가하다는 한계를 염두해두어야 합니다.

- **정밀도 손실 및 양자화 오차**: 고정소수점은 유효한 소수부 비트 수가 한정되어 있어 표현 해상도가 떨어지므로, 나타낼 수 없는 미세한 값은 양자화 오차로
  처리됩니다. 예를 들어 0.1(10진수)을 이진 고정소수점(Q-format)으로 표현할때 딱 떨어지지 않는 경우가 많아 근사값으로 저장해야 합니다. 고정소수점에서는
  인접한 두 수 간격이 고정되는데, 이 간격이 부동소수점보다 훨씬 크기 때문에 연산 중 반올림/절삭 오차가 더 두드러질 수 있습니다. 즉, 실제 아날로그 값과
  디지털 표현값의 차이(오차)가 부동소수점보다 크게 나타날 수 있습니다. 예를 들어 16비트 고정소수점(Q15.1 형태 등)으로 매우 작은 증분을 반복해 더하면
  , 그 증분이 최소 단위보다 작다면 계산 결과에 전혀 반영되지 않는 현상도 생깁니다. 이런 정밀도 한계 때문에 고정소수점 연산 결과는 부동소수점 대비 오차가
  누적되기 쉽고, 특히 계산 단계가 많거나 긴 필터 연산 등에서는 신호과 왜곡될 수 있습니다. 따라서 고정 소수점 시스템에서는 필요한 정밀도를 확보하기 위해
  스케일링 조정과 올바른 반올림 처리가 반드시 수반되어야 합니다.

- **스케일 관리의 복잡성**: 고정소수점으로 연산할때는 프로그래머/디자이너가 직접 소수점 위치(스케일)를 관리해야 하므로 구현이 까다롭습니다. 각 연산 후에는
  결과를 적절히 shift 하거나 비트 길이를 확장하지 않으면, 오버플로우나 정밀도 손실이 발생하기 쉽습니다. 예를 들어 고정소수점 A(Qm.n 형식)와 B(Qp.q)형식
  를 곱하면 일시적으로 비트수가 늘어나며 가수부 비트도 증가합니다. 이때 결과를 저장하기 전에 미리 정해둔 스케일로 자리수를 맞춰주지 않으면 상위 비트가 잘려나가거나
  엉뚱한 값이 됩니다. 고정소수점 DSP 개발에서는 연산마다 수를 최대한 키워서(shift left) 계산 범위를 높이고, 결과가 범위를 넘지 않도록 다시 줄이는(shift right)
  과정을 반복적으로 수행합니다. 이러한 수동 조정이 제대로 이루어지지 않으면, 비트 손실로 중요한 미세 정보가 사라지거나 overflow를 일으켜 결과가 엉망이
  될 수 있습니다. 따라서 고정소수점 시스템에서는 스케일링 전략을 정교하게 설계해야 하며, 모든 연산 경로에 대해 최악의 경우를 가전한 비트 산출 분석이
  필요합니다. 이런 이유로 고정소수점 연산은 부동소수점 연산에 비해 개발자가 신경써야 할 부분이 많고 실수하기 쉽습니다.

  

### 부동소수점: 개념 및 사례
부동소수점 방식은 소수점의 위치를 고정하지 않고 자유롭게 "떠다니도록(floating)" 함으로써, 가수(mantissa)와 지수(exponent)라는 값을 사용해 실수를 표현하는 방법입니다.
고정소수점과 비트 구성 방식이 완전히 다르며, 현재 대부분의 컴퓨터 시스템에서 사용되는 실수 표준은 IEEE 754 부동소수점 표준입니다. 
예를 들어 32비트 단정밀도(single precision) IEEE 754 부동소수점 형식은 아래와 같은 비트 구조를 갖습니다.

![BitOperations](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/General_floating_point_ko.svg/1000px-General_floating_point_ko.svg.png)
[그림2. IEEE 부동소수점 방식]

- 부호부(Sign): 1비트 (양수는 0, 음수는 1)

- 지수부(Exponent): 8비트 (정규화된 지수값에 bias 127을 더해 저장)

- 가수부(Mantissa): 23비트 (정규화된 소수 부분의 값 저장)

즉, 32비트 부동소수점은 ［부호 1］［지수 8］［가수 23］ 비트로 구성됩니다. 부동소수점 표현에서는 실제 실수 값을 ±(1.M) × 2^E 형태로 나타내는데, 여기서 1.M은 가수부에 해당하는 유효숫자이고 E는 2의 지수입니다. 
(가수부에는 정규화된 2진수에서 소수점 이하 부분만 저장하고, 정수부는 항상 1로 가정되는 숨겨진 비트(hidden bit) 방식입니다.)

이 방식을 7.625 같은 숫자에 적용해보면 과정은 다음과 같습니다. 7.625(10)를 2진수로 변환하면 111.101(2)인데, 부동소수점 표현을 위해 정규화를 수행하여 소수점 왼쪽에 한 자리의 1만 남기면 1.11101(2) × 2^2 형태가 됩니다. 
여기서 지수 E=2이고 가수부 M=1.11101(2)입니다. 지수부에는 E 값(2)에 bias 127을 더한 129를 저장하는데, 129를 2진수 8비트로 표현한 값이 10000001입니다. 
가수부에는 정규화된 1.11101에서 1.을 제외한 소수 부분 11101을 좌측부터 채워 넣고, 남는 비트는 0으로 채웁니다. 
부호는 양수이므로 0입니다. 따라서 7.625의 단정밀도 부동소수점 표현 비트열은 다음과 같습니다:

```
0 10000001 11101000000000000000000 (2)

```
(부호 0 = 양수, 지수 10000001₂ = 129 = 2+127, 가수 11101...₂)

위 비트열을 해석해보면, 부호 0이 양수, 지수부 10000001₂는 편향 지수 129(실제 지수 2)를 의미하며, 가수부가 나타내는 2진수는 1.11101₂ (소수부에 11101을 채우고 나머지는 0)입니다. 
이를 종합하면 값은 +1.11101₂ × 2^2 = 111.101₂ = 7.625로 복원됩니다.

부동소수점의 가장 큰 장점은 동일 비트로 훨씬 넓은 실수 범위를 표현할 수 있다는 것입니다. 예를 들어 32비트 고정소수점에서 정수부 15비트로 표현할 수 있는 최대 값은 약 2^15≈3.3×10^4 수준이지만, 
32비트 부동소수점에서는 지수부 8비트를 활용하여 최대 2^127 ≈ 1.7×10^38 (약 ±3.4×10^38)까지 표현할 수 있습니다.
최소로는 지수가 음수가 되어 2^-126 정도의 매우 작은 숫자 (~1.4×10^-45)까지도 나타낼 수 있습니다. 결과적으로 부동소수점은 한정된 비트로 정수부에 할당되는 자릿수를 동적으로 변화시킬 수 있으므로(소수점 위치가 움직이므로) 
고정소수점보다 훨씬 큰 수나 작은 수를 자유롭게 다룰 수 있습니다. 또한 23비트의 가수부는 약 7자리 정도의 10진수 유효숫자를 유지하므로, 과학/공학 계산에서 충분한 정밀도를 제공합니다.

반면, 부동소수점의 가장 큰 단점은 연산 결과의 미세한 부정확성입니다. 2진 형태로 실수를 표현하는 근본적 한계 때문에, 소수 부분이 무한히 반복되는 실수는 컴퓨터가 정확하게 저장하지 못하고 가장 근사한 값으로 표현합니다.
예를 들어 0.3(10)은 2진수로 0.0100110011...₂처럼 끝없이 반복되는 패턴을 가져서, 실제 값과 정확히 일치하도록 2진 유효숫자로 저장할 수 없습니다. 결국 제한된 23비트 가수부에는 0.3의 근삿값만 저장되고, 
이로 인해 계산 과정에서 아주 작은 오차가 발생할 수 있습니다. 실제로 부동소수점 오차의 유명한 사례로 파이썬에서 0.1 + 0.2를 해보면 0.3이 아닌 아래와 같은 결과가 나타납니다

```python
print(0.1 + 0.2)        # 0.30000000000000004
print(0.1 + 0.2 == 0.3) # False

```

가 되어버리므로, 0.3과 비교하면 False가 출력됩니다. 이처럼 부동소수점에서는 연산상 보이지 않는 반올림 오차(round-off error)가 누적될 수 있고, 특히 두 실수를 직접 비교할 때 주의가 필요합니다. 
(참고로 파이썬 float는 IEEE 754 배정밀도 64비트 형식을 사용하기 때문에 여기 예시로 든 32비트 단정밀도보다 정밀도가 높지만, 동일한 원리로 소수 오차가 발생할 수 있습니다.)

| **특징**    | **고정소수점**                 | **부동소수점**                          |
| --------- | ------------------------- | ---------------------------------- |
| **표현 방식** | 부호 + 정수부 + 소수부 비트를 고정 분할  | 부호 + 지수 + 가수 (정규화된 유효숫자)           |
| **표현 범위** | 제한적 (예: 32비트로 ±3×10^4 정도) | 매우 넓음 (예: 32비트로 ±3.4×10^38 정도)     |
| **정밀도**   | 고정된 소수 자릿수 (절대 오차 일정)     | 고정된 유효숫자 자릿수 (상대 오차 일정, 단정밀도 ~7자리) |
| **연산 속도** | 매우 빠름 (정수 연산 재사용)         | 비교적 느림 (복잡한 계산, 전용 FPU 사용)         |
| **장점**    | 구현 간단, 처리 속도 빠름           | 광범위한 값 표현, 충분한 정밀도                 |
| **단점**    | 표현 가능한 범위 제한, 정밀도 낮음      | 근사 오차 발생 가능, 구현 복잡                 |

일반적으로 정수 연산은 동일한 클럭 속도에서 부동소수점 연산보다 빠르며, 과거에는 부동소수점 연산 기능이 제한된 소형 프로세서도 많았습니다. 
하지만 현대의 대부분 시스템은 부동소수점 연산을 위한 **전용 하드웨어(FPU)**를 갖추고 있고, 프로그래밍 언어도 기본 실수 타입으로 부동소수점을 제공합니다. 
그만큼 부동소수점 방식이 표준으로 정착되어 있으며, 과학기술 계산처럼 매우 큰 수나 작은 수가 필요한 분야에서 없어서는 안 될 존재입니다.
고정소수점 방식은 특수한 용도(예컨대 부동소수점 연산 장치가 없는 임베디드 시스템이나, 소수점 이하 자릿수가 일정한 금액 계산 등)에서 제한적으로 활용될 뿐, 
대부분의 일반적인 실수 계산에는 부동소수점이 사용됩니다

### 개발자를 위한 실전 팁
앞서 살펴본 것처럼 부동소수점은 현실의 실수 계산에서 편리하고 강력하지만, 정밀도 한계로 인해 생기는 미묘한 오차를 이해하고 다루는 것이 중요합니다. 
컴퓨터 공학 입문자와 개발자를 위해 부동소수점 숫자를 사용할 때 기억해두면 좋은 팁을 정리하면 다음과 같습니다:

1. **부동소수점 비교 시에는 epsilon 허용 오차를 두기**: 두 실수 값을 직접 ==로 비교하면 앞서 본 0.1+0.2==0.3 사례처럼 의도치 않게 False가 나올 수 있습니다.
   따라서 실수 비교에는 일정 범위의 허용 오차를 두고 판단해야 합니다. Python 내장 함수 math.isclose()는 이러한 용도로 두 부동소수점이 거의 같은지 확인하는 편리한 방법을 제공합니다.
   예를 들어:
   ```python
   import math
   print(math.isclose(0.1+0.2, 0.3, rel_tol=1e-9))  # True (허용 오차 범위 내에서 동일
   ```
   위와 같이 math.isclose()는 상대 오차 rel_tol (또는 필요시 절대 오차 abs_tol) 내에서 두 값이 가까우면 True를 반환합니다.
   혹은 언어에 따라 epsilon 값을 직접 정의하여 abs(a - b) < epsilon 형태로 비교하는 방법도 활용할 수 있습니다.
   부동소수점의 미세 오차는 보통 10^-15 수준(64비트 기준)이므로, 이 정도 작은 범위를 두고 비교하면 대부분의 경우 올바르게 판단할 수 있습니다.

2. **금융 계산 등에는 Decimal 등 대체 자료형 사용 고려**: 화폐 단위처럼 10진 소수의 정확도가 요구되는 경우라면 이진 부동소수점을 그대로 쓰기보다,
   언어에서 제공하는 소수 전용 자료형을 사용하는 것이 좋습니다. 파이썬의 경우 decimal 모듈이 10진수 고정소수점/부동소수점 연산을 지원하며,
   0.1과 0.2 같은 값도 정확히 표현하고 연산할 수 있습니다. 실제로 Decimal('0.1') + Decimal('0.2')의 결과는 오류 없이 **Decimal('0.3')**이 됩니다.
   이처럼 decimal을 쓰면 부동소수점 특유의 0.3000004와 같은 현상을 피할 수 있어 금융계산에 유용합니다. 또한 fractions.Fraction 클래스는 분수를 사용해 1/3 같은 무한소수를 정확하게 표현하는 방법도 제공합니다.
   계산 성능은 부동소수점에 비해 떨어지지만, 정밀도가 중요한 프로그램에서는 이런 대안을 고려해야 합니다.


### 부동 소수점 방식의 대표적 활용 사례
- **과학 계산 및 HPC(고성능 컴퓨팅)**: 부동소수점은 아주 큰 수나 작은 수를 동시에 다루는 과학기술 분야에서 필수적입니다. 지수부를 갖는 표현 덕분에 동적 범위가
  넓어, 가상 시뮬레이션이나 천체 물리 시뮬레이션 같은 경우 부동소수점 없이는 현실적인 계산이 어렵습니다. 예를 들어 기상 예측 모델은 수치 범위가 방대하여 IEEE 754
  배정밀도 부동소수점이 제공하는 극한의 범위가 필요합니다. 실제로 대부분의 과학/공학용 소프트웨어와 슈퍼컴퓨터 애플리케이션은 부동소수점 연산에 크게 의존적이며, HPC
  분야의 계산은 거의 모두 부동소수점으로 수행됩니다. 부동소수점 연산 성능을 가리키는 FLOPS(Floating Point Operations Per Second) 지표가 슈퍼컴퓨터 성능의
  척도로 쓰이는 것처럼, 과학기술 연산에서 부동소수점은 표준입니다.

- **컴퓨터 그래픽스 및 게임 개발(현대)**: 현대의 그래픽 엔진과 GPU는 대부분 부동소수점 연산을 기반으로 동작합니다. 실시간 3D 렌더링에서는 빛의 강도처럼 크기가 매우
  다른 값들을 계산하거나 Mesh 좌표 변환 등을 위해 부동소수점의 넓은 표현 범위와 연산 편의성이 필요합니다. 사실상 모든 현대 GPU는 파이프라인 전체에 32bit 부동소수점
  연산을 지원하며, 대부분의 그래픽 셰이더 계산이 부동소수점으로 이루어집니다. 예를 들어 OpenGL/Direct3D 같은 그래픽 API에서 정점 좌표, 색상, 조명 계산에
  float 타입이 기본으로 사용됩니다. 하드웨어적으로도 GPU는 부동소수점 연산에 특화되어 있어 대규모 병렬 부동소수점 연산(FLOPS)을 처리함으로써 고해상도 게임,
  3D 시각화, VR/AR 등에서 사실적이고 복잡한 그래픽스를 실시간적으로 구현할 수 있습니다. (과거엔 PS3/Xbox 360 세대부터 게임 개발에 부동소수점이 본격 도입되어,
  현재 게임 산업에서는 부동 소수점 연산이 사실상 필수 표준입니다.)

- **인공지능/머신러닝**: 딥러닝 모델의 훈련(training)에는 수백만 개의 가중치에 대한 미세한 업데이트와 큰 범위의 그레디언트 계산이 필요하기 때문에, 단정밀도(32-bit)
- 부동소수점이 오랫동안 사용되어 왔습니다. 예를 들어 대부분의 GPU 연산은 기본적으로 FP32 부동소수점으로 이루어지며, 엔비디아의 GPU들은 모두 FP32와 FP16
  연산을 지원해 딥러닝 연산 가속에 활용됩니다. 부동소수점의 충분한 정밀도와 범위 덕분에 신경망 학습시 작은 오차 누적을 감내하면서 안정적으로 수렴을 이끌수 있습니다.
  최근에는 메모리 절약과 속도를 위해 반정밀도(FP16)나 bfloat16까지 활용하지만, 혼합 정밀도 기법으로라도 부동소수점 연산은 계속 활용됩니다. 또한 과학 데이터
  분석, 금융 시뮬레이션(리스크 계산)등 데이터 사이언스 분야 전반에서도 부동소수점 연산이 널리 쓰이고 있어, 대용량 행렬 연산이나 통계 처리 등을 정확하면서 유연하게
  수행하고 있습니다.

### 부동소수점 방식의 주요 엣지 케이스 (한계 및 주의점)
- **대표값의 표현 오차**: 부동소수점은 많은 실수를 근사하여 표현하기 때문에, 10진 소수로 딱 떨어지지는 값도 2진 부동소수점으로는 미세한 오차가 발생합니다.
  예를 들어 10진수, 1.1이나 0.1은 2진수로 변환하면 무한 소수가 되므로 컴퓨터는 근사값으로 저장할 수 밖에 없습니다. 32비트 부동소수점(float)의 경우
  0.1은 내부적으로 0.10000000149011612… 정도로 저장되며, 0.1을 정확히 표현하지 못한 작은 오차가 존재합니다. 이로 인해 0.1 + 0.2 = 0.3 같은
  산술이 컴퓨터에서는 정확히 성립하지 않을 수 있습니다.

  실제로 Python이나 C++에서 0.1 + 0.1 + 0.1 == 0.3 를 계산하면 False가 나오는데, 0.1의 이진 근사값을 세 번 더한 결과가 0.30000000000000004
  처럼 약간 큰 값이 되기 때문입니다. 이러한 표현 오류는 부동소수점 사용 시 피할 수 없는 문제이며, 특히 10진 기반 계산(금융 등)에서는 치명적일 수 있습니다.
  다행히 대부분 언어의 출력은 이러한 미세한 오차를 반올림하여 표시해주지만, 내부적으로는 오차가 있다는 것을 항상 염두에 두어야 합니다.

- **연산 순서에 따른 오차 누적과 상쇄(cancellation)**: 부동소수점은 유효숫자 자릿수가 한정되어 있어, 연산을 반복하거나 큰 수와 작은 수를 함께 계산할 때
  상대적 오차가 누적될 수 있습니다. 예를 들어 아주 큰 수에 아주 작은 수를 더하면 작은 수의 변화가 결과에 반영되지 않는 경우가 있습니다. 이는 큰수에 맞춰
  지수부가 정렬되면서 작은 수의 유효숫자가 잘려나가기 때문인데, 단정밀도(float)에서는 약7자리 정도의 십진 유효숫자만 유지되므로 1,000,000,000 + 0.000001
  같은 계산에서 작은 값이 무시될 수도 있습니다. 또한 카타스트로픽 캔슬래이션(catastrophic cancellation)이라 불리는 현상도 있습니다.

  거의 비슷한 두 수의 차를 구하면 앞자리 상당수가 상쇄되어 남는 유효숫자가 매우 줄어드는 문제입니다. 예를 들어 123456.7와 123456.6을 부동소수점으로
  뺄 경우 실제 차이는 0.1이지만, 두 수 각각의 근사값 오차가 결과를 크게 좌우하여 정밀도가 급격히 떨어지는 일이 발생합니다. 이러한 상쇄로 인해 유효 숫자가
  사라지면 결과값은 신뢰할 수 없는 수준의 노이즈만 남게 됩니다. 이처럼 부동소수점 계산에서는 연산의 순서나 값의 크기 차이에 따라 오차 분포가 달라집니다.
  누적 합계를 구할 때 작은 값들부터 더하는 Kahan Summation 알고리즘 등을 쓰는것도 이 때문이며, 일반적으로 부동소수점으로 수치를 다룰때는 계산 순서를
  신중히 선택해야 합니다. 마지막으로, 위의 예시에서 설명했듯이 부동소수점 비교 연산에서도 직접 == 비교는 피하고, 허용 오차 범위(epsilon)를 두고 비교하는
  습관이 필요합니다.

- **특수 값(Inf, NaN등) 및 플랫폼별 차이**: 부동소수점 표준(IEEE 754)에는 일반 실수 이외에 특수한 값들이 존재합니다. 계산 중 결과가 표현 범위를 넘으면
  무한대(Infinity)로 표시되고, 0으로 나누거나 정의되지 않은 연산을 하면 NaN(Not a Number)이라는 값을 얻습니다. 예를 들어 1e308 * 1e308 (배정밀도에서
  최댓값을 넘는 곱셈)은 무한대로 처리되고, 0.0/0.0은 NaN이 됩니다. 이러한 특수 값들은 프로그램 로직에 따라 별도 처리가 필요하며, 제대로 인식하지 못하면
  버그로 이어질 수 있습니다. 또한 +0.0 과 -0.0처럼 부동소수점에는 부호가 다른 두 가지 0이 존재하는데, 이는 주로 극한 연산이나 특정 함수 계산에서 부호 정보를
  보존하기 위함이지만 개발자에겐 혼란을 줄 수 있습니다. 예를 들어 일부 언어에서 -0.0을 출력하면 "-0.0"으로 표시되지만, 수치를 비교하면 -0.0 == 0.0은
  True로 간주됩니다. 더불어 서로 다른 하드웨어/컴파일러 에서 부동소수점 연산의 반올림 모드 차이나 최적화 차이로 미세한 결과 오차가 생겨, 동일한 계산이라도
  플랫폼에 따라 마지막 비트가 달라질 수 있습니다.

  이러한 이유로 분산 시스템에서 부동소수점 결과를 직접 비교하면 문제가 되기도 합니다. 요약하자면, 부동소수점 사용 시에는 오버플로우/언더플로우로 인한 특수값 처리
  , -0 처리, 그리고 플랫폼 간 일관성 등을 모두 고려해야 합니다. 필요하다면 임의 정밀도 연산(BigDecimal 등)이나 고정소수점 대안으로 전환하는 것도 검토해볼만
  합니다.

  

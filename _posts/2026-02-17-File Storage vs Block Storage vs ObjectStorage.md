---
layout: post
title: "File Storage vs Block Storage vs ObjectStorage"
date: 2026-02-17 22:00:00 +0900
categories: [Storage ,OS, CS, DevOps]
author: cotes
published: true
---

## 파일 스토리지 vs Block Storage vs Object Storage
파일 스토리지, 블록 스토리지, 오브젝트 스토리지는 데이터를 저장하고 구성하는 방식이 각각 다르며, 이를 통해 서로 다른 장단점과 용도를 가집니다. 
예를 들어 파일 스토리지는 계층적 디렉토리 구조에 파일을 저장하고, 블록 스토리지는 데이터를 고정된 크기 블록으로 쪼개 저장하며, 오브젝트 스토리지는 
메타데이터를 포함한 개별 객체 단위로 평면 공간에 저장합니다. 이 글에서는 각 스토리지 구조의 동작 원리, 주요 사용 사례, 장단점과 실제 사용사례를 살펴보겠습니다.

![BitOperations](https://substackcdn.com/image/fetch/$s_!Zwv8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F3d09290b-ae20-494b-aeab-9fc2b095ff7f_2154x798.png)
그림1. 스토리지 비교 <출처: ByteByteGo Newsletter>

### File Storage

작동원리: 파일 스토리지는 데이터를 파일과 폴더 계층으로 저장합니다. 디렉토리 트리를 거쳐 파일의 경로(path)를 찾아 데이터에 접근하며, 파일 이름과 확장자 등을
메타데이터로 사용합니다. 일반적으로 NAS(Network Attached Storage)나 공유 파일 서버 형태로 구현되어 NFS, SMB/CIFS 같은 파일 공유 프로토콜을 사용합니다.
예를 들어 리눅스에서는 NFS로, 윈도우에서는 SMB로 네트워크 파일 시스템을 구성하여 다수의 클라이언트가 중앙 파일 서버를 마운트해쓸수 있습니다. k8s 환경에서는
NFS나 GlusterFS/CephFS 같은 파일 시스템을 Persistent Volume으로 사용해 컨테이너 간에 읽기/쓰기 다중(ReadWriteMany)엑세스를 지원할 수 있습니다.
또한 HPC, 빅데이터, AI/ML 워크로드에서는 노드 수백 ~ 수천대가 동시에 하나의 파일 시스템에 접근해야 하는데 이떄 등장하는 것이 분산파일시스템(DFS)입니다.
대표적으로 Lustre, Hadoop등이 있습니다. 이에 대한 내용은 따로 다루도록 하겠습니다.

**사용사레**: 파일 스토리지는 전통적인 파일 공유, 사용자 디렉토리, 콘텐츠 관리, 문서 저장소 등에 적합합니다. 웹서버의 정적 콘텐츠, 빌드 서버의 출력, 로그 수집
파일등 다수의 서버가 동일 파일에 접근해야 하는 경우 자주 사용됩니다. 특히 여러 애플리케이션 노드가 동시에 같은 디렉토리에 읽기, 쓰기를 수행해야 하는 경우(Ex:웹 클러스터의
동기화 파일, 중앙 로그 저장소)에 효과적입니다.

**장점**: 파일 스토리지는 OS에 기본적인 파일 시스템 형태이므로 사용자에게 친숙하고 관리가 쉽습니다. POSIX 호환 I/O와 디렉토리 구조로 손쉽게 파일을 생성.삭제.이동할 수 있으며,
대부분의 운영체제와 애플리케이션이 바로 사용할 수 있습니다. 또한 NFS,SMB와 같은 표준 프로토콜을 활용해 여러 서버가 동시에 동일한 파일 시스템을 마운트 할 수 있어 데이터 
공유에 용이합니다.

**단점**: 파일 시스템은 트리 구조의 확장성에 한계가 있습니다. 저장 용량을 늘릴때는 일반적으로 파일 서버 노드를 추가해야 하며, 개별 시스템의 수평확장이 필요합니다. 대용량 데이터를 
계층적으로 관리해야 하므로 메타데이터 관리 오버헤드가 커져 대량 파일의 검색 성능이 저하되기 쉽습니다. 또한 파일 락이나 동기화가 필요한 멀티-노드 환경에서는 락 경쟁이 발생할 수 있어
복잡성이 높아집니다.

AWS에서는 Amazon EFS(Elastic File System, NFS 방식)을 통해 여러 EC2 인스턴스가 동시에 파일 시스템을 공유할 수 있습니다.
Azure에서는 Azure Files(SMB/NFS 지원)와 고성능 Azure NetApp Files가 있습니다. GCP에서는 Filestore 서비스를 통해 NFS 기반 파일 스토리지를 제공합니다.
Kubernetes에서는 NFS나 CephFS, GlusterFS 등을 CSI 드라이버로 이용해 ReadWriteMany 접근을 제공하며, 특히 Ceph 기반 Rook-CephFS를 활용하면 Kubernetes
네이티브로 파일 스토리지를 구성할 수 있습니다.


### Object Storage
![BitOperations](https://substackcdn.com/image/fetch/$s_!eUyv!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F297af517-690b-4413-a819-fa86458c81ff_800x447.png)
그림 2. Object Storage(AWS S3)<출처: https://hayksimonyan.substack.com/p/object-storage-blobs-explained-for>

**작동원리**: 오브젝트 스토리지는 데이터를 오브젝트 단위로 저장하며, 각 오브젝트는 고유 식별자(ID)와 풍부한 매타데이터를 가집니다. 전통적인 파일 시스템과 달리 디렉토리 계층을
사용하지 않고, 전역적인 평면(flat) 네임스페이스(버킷,컨테이너)에 모든 객체를 저장합니다. 각 오브젝트는 데이터를 포함하는 실제 파일(ex: 이미지, 동영상)과 파일을 설명하는 
key-value 형식의 메타데이터로 구성됩니다. AWS S3의 경우 s3://<bucket>/<path>/<filename>과 같은 URL이 객체의 위치가 되며, S3 REST API를 통해 구성됩니다.
스토리지 내부적으로는 분산 시스템 상의 여러 서버에 데이터를 나누어 저장하므로, 수평 확장과 내구성 면에서 유리합니다.

**사용 사례**: 오브젝트 스토리지는 대량의 비정형 데이터를 저장.관리하는데 적합합니다. 정적 콘텐츠(이미지, 비디오 등 미디어 파일), 애플리케이션 백업 및 아카이브, 로그 및 IoT
센서 데이터, 빅데이터 저장소 등에 활용됩니다. 특히 클라우드 네이티브 애플리케이션이나 머신러닝 학습용 데이터, 콘텐츠 딜리버리 네트워크(CDN) 등의 경우 오브젝트 스토리지가
편리합니다. AWS의 S3, Azure의 Blob Storage, GCP의 Cloud Storage 등이 대표적이며, Ceph에서는 RadosGW를 통해 S3/Swift 호환 오브젝트 스토리지를 제공합니다.

**장점**: 거의 무제한에 가까운 확장성을 제공합니다. 개별 장치 한계를 넘어 자유롭게 노드를 추가하여 용량을 늘릴수 있습니다. 데이터와 메타데이터가 객체에 함께 저장되므로 검색
기능이 강력하고, 메타데이터 기반 조회가 가능합니다. 또한 HTTP(S) 기반의 표준 API로 접근할수 있어 사용 편의성이 높고, 사용한 만큼 과금되는 종량제 요금제 모델로 비용효율적입니다.


AWS S3, Azure Blob, GCP Cloud Storage 등 주요 클라우드 객체 스토리지 서비스가 오브젝트 스토리지를 제공합니다. Kubernetes 환경에서는 오브젝트 스토리지를 네이티브로
지원하지 않지만, MinIO 같은 S3 호환 솔루션이나 Rook-Ceph RGW를 배포하여 애플리케이션에서 오브젝트 API를 사용할 수 있습니다. 또한 CSI 드라이버(s3.csi.k8s.io 등)
를 통해 S3 API를 Persistent Volume으로 마운트하는 방식을 활용하기도 합니다.

### Block Storage
**작동 원리**:  블록 스토리지는 데이터를 크기가 일정한 블록 단위로 분할하여 저장합니다. 각 블록은 고유 식별자를 가지며, 스토리지 시스템이 효율적인 위치에 블록들을
분산하여 저장한뒤 필요시 블록들을 재조립하여 데이터를 복원합니다. 블록 장치는 운영체제로부터 원시 디스크처럼 보이므로, 마운트하거나 파일 시스템을 생성하여 일반 디스크처럼 사용할 수 
있습니다. 보통 ISCSI, Fibre Channel, Virtio 등 블록 디바이스 인터페이스를 통해 서버에 연결됩니다.

블록장치는 I/O 요청 순서가 성능에 크게 영향을 주며, 커널은 메모리 캐시 및 요청 정렬 알고리즘으로 성능을 극대화 합니다. 여기서 사용되는 알고리즘이 Elevator 알고리즘입니다.
이는 디스크 헤드가 이동하는 방향을 고려해 요청을 정렬함으로써 디스크 성능을 높입니다.

### Elevator Algorithm
엘리베이터 알고리즘은 디스크 헤드를 한 방향으로 이동하면서 요청을 처리하고, 끝에 도달하면 방향을 바꿔 다시 처리하는 방식입니다. 엘리베이터가 위로 가면서 요청 층을 처리하고
끝층에 가면 다시 아래로 내려오는것과 동일합니다. 핵심은 디스크 헤드 거리 최소화 -> Seek Time 감소 -> 전체 I/O 성능 향상
HDD에서는 헤드 이동 비용이 크기 때문에 매우 중요합니다.

수학적 정의
- 요청 집합: R={r1​,r2​,...,rn​}
- 현재 헤드 위치: H0​
- 처리순서: Rsorted​=sort(R)
-SCAN 알고리즘의 총 이동 거리: $C = \sum_{i=1}^{n} |H_i - H_{i-1}|$

정렬 후, 현재 방향 기준으로 분할, 한쪽 끝까지 처리 후 역방향 진행

### SCAN 
```
0 ----------------------------------- 199
            ^
           Head
```
- 한 방향으로 끝까지 감
- 방향을 바꿔 반대쪽 처리


### C-SCAN (Circular SCAN)
- 한 방향으로 끝까지 감
- 끝에서 바로 반대쪽 끝으로 점프
- 항상 한 방향으로만 처리

예를 들어 아래 조건일때
- 디스크 트랙: 0 ~ 199
- 현재 헤드: 50
- 요청: {10, 22, 20, 2, 40, 6, 38}

1. 정렬: {2, 6, 10, 20, 22, 38, 40}
현재 위치 50 -> 왼쪽으로 이동한다고 가정합니다.

2. 처리순서 (SCAN)
50 -> 40 -> 38 -> 22 -> 10 -> 6 -> 2

3. 총 이동거리
∣50−40∣+∣40−38∣+∣38−22∣+∣22−20∣+∣20−10∣+∣10−6∣+∣6−2∣

= 10 + 2 + 16 + 2 + 10 + 4 + 4
= 48

FCFS와 비교

FCFS 순서:

50 → 10 → 22 → 20 → 2 → 40 → 6 → 38

총 이동 거리 = 208

➡ SCAN이 훨씬 효율적

리눅스 2.4 시절에는 실제로 SCAN 기반 스케줄러를 사용했습니다.

그러나 현대 리눅스에서는 다음 스케줄러들이 등장합니다:

- CFQ

- Deadline

- NOOP

- MQ-Deadline

- BFQ

SSD 등장 이후에는
헤드 이동 개념이 거의 의미가 없기 때문에
엘리베이터의 중요성은 감소했습니다.

### 블록 장치 접근 User Space vs Kernel Spcae
사용자 공간 접근
- /dev/sd1 등과 같은 특수 파일로 접근합니다.
- 파일 이름이 중요한것이 아니라, major/minor 번호가 디바이스 드라이버와 파티션을 식별합니다.

### 커널 내부 구조 - 핵심 자료구조(구조체)
| 구조체명              | 역할                           |
| ----------------- | ---------------------------- |
| **gendisk**       | 디스크 전체 정보 (queue, 파티션, ops)  |
| **hd_struct**     | 개별 파티션 정보                    |
| **block_device**  | 장치 파일이 열릴 때 생성되는 커널 대표 구조    |
| **buffer_head**   | 메모리에 올라온 블록 단위 데이터           |
| **bio**           | I/O의 기본 단위 — 실제 I/O 요청       |
| **bio_vec**       | 메모리 상의 I/O 세그먼트 정보           |
| **request**       | 소프트웨어 큐에 있는 요청 단위 구조         |
| **request_queue** | 요청 큐 — 정렬, 병합, Elevator 등 포함 |

이 구조체들은 블록 장치가 열리고, 요청이 생성되고, 디스크에 도달해 처리될때까지 연결된 흐름을 구성합니다.

참고 글 -> (https://chatgpt.com/c/69941f18-5f9c-8321-88cf-7d9c8882df03)
알고리즘 코드 -> (https://lxr.linux.no/linux/block/elevator.c#L170)

![BitOperations](https://yannik520.github.io/blkdevarch_files/gendisk26.png)
그림3. block device의 구조체간의 관계

### 4. I/O 요청이 어떻게 처리되는가
1. bio 생성
애플리케이션이 데이터를 읽거나 쓰면, 커널은 먼저 bio 구조체를 생성합니다. bio는 여러 메모리 세그먼트를 묶어서 하나의 I/O 요청 단위로 관리합니다.

2. request Queue로 제출
bio는 submit_bio API를 통해 블록 계층으로 전송되고, 내부적으로 __make_request를 통해 request 큐에 놓입니다.

3. Elavator & 병합
- Elavator 알고리즘이 요청을 재정렬
- 인접한 요청끼리 병합 가능하면 병합

이렇게 해서 디스크 헤드 움직임 최소화 및 대량 I/O 처리 성능을 높입니다.

4. 실제 디바이스로 전달
일정 시간이 지나거나, 큐가 차면 블록이 unplug 되며 request_fn 드라이버로 I/O가 전달됩니다.

5. 장치 열기와 파일 시스템 마운트
- 블록 장치는 일반 파일처럼 열 수 있고 커널은 이를 inode로 처리합니다.
- blkedev_open -> 요청 큐 등록 -> bio 생성 -> I/O 전송.. 의 흐름으로 동작합니다.

즉, 파일시스템이 블록 장치를 마운트하면
1. 커널이 special inode를 생성
2. inode에 block_device 연결
3. 이후 병합/전달/정렬/처리 루프가 실행됩니다.

![BitOperations](https://yannik520.github.io/blkdevarch_files/cg_bdev26.png)
그림4. file open process diagram


**사용사례**: 블록 스토리지는 고성능과 낮은 지연이 필요한 트랜잭션 처리 워크로드에 적합합니다. 데이터베이스, 컨테이너(영속 볼륨), 가상머신의 부팅 디스크 등에 주로 사용됩니다.
또한 대량의 순차,랜덤 읽기.쓰기가 필요한 미디어 인코딩, 캐싱, 분석 작업에서도 많이 활용됩니다.

**장점**: 블록 스토리지는 데이터를 디스크 수준에서 직접 엑세스하므로 I/O 지연이 낮고 성능이 우수합니다. 개별 블록 단위로 변경이 가능하여 데이터 수정 시 전체 파일을 재작성하지
않아도 되고, RAID등으로 구성시 높은 처리량을 얻을 수 있습니다. 또한 파일시스템을 선택하여 포맷할 수 있기 때문에 다양한 OS 및 포맷과 호환됩니다.

**단점**: 비용이 비교적 높습니다. 전통적으로 SAN(Storage Area Network)이나 고성능 SSD등을 필요로 하기 때문에 구축.운영 비용이 큽니다. 메타데이터가 제한적(파일시스템 수준)
이여서 데이터 자체의 설명 정보가 적고, 스토리지 공간을 미리 할당해야 하므로 사용량에 상관없이 요금이 과금될 수 있습니다. 또한 블록 디바이스는 특정 호스트에 연결되어야 하기 때문에
(EBS 볼륨을 하나의 EC2 인스턴스에만 연결) 다중 노드 공유에 제한이 있으며, 파일 시스템을 통해서만 다수 엑세스 시 락 이슈가 발생할 수 있습니다.

AWS의 EBS(Elastic Block Storage)는 EC2 인스턴스에 부착 가능한 블록 볼륨을 제공합니다. Azure에서는 Managed Disk(표준, 프리미엄 SSD, Ultra Disk 등)가 VM에 
블록 디스크를 제공하며, Azure Storage 계정의 Disk Storage 옵션도 있습니다. GCP는 Persistent Disk(PD)를 통해 Compute Engine에 블록 스토리지를 제공합니다.
kubernetes에서는 클라우드 제공자의 블록 볼륨(CSI)으로 PVC를 사용합니다. 예컨데 AWS EBS CSI 드라이버, Azure Disk CSI, GCE PD CSI 등을 통해 ReadWriteOnce
블록 볼륨을 마운트하여 사용할 수 있습니다. 특히 Ceph의 RBD(RADOS Block Storage)와 같은 소프트웨어 정의 스토리지는 단일 클러스터에서 블록 스토리지를 제공하기도 합니다.


| 구분         | 파일 스토리지                                          | 블록 스토리지                                                     | 오브젝트 스토리지                                          |
| ---------- | ------------------------------------------------ | ----------------------------------------------------------- | -------------------------------------------------- |
| **데이터 구조** | 계층적 디렉터리 트리에 파일 단위 저장                            | 고정 크기 블록(볼륨) 단위로 저장                                         | 전역 평면 네임스페이스에 객체 단위 저장 (ID+메타데이터)                  |
| **접근 방식**  | POSIX 파일 I/O (NFS, SMB/CIFS) 사용, 경로 기반 접근        | 블록 장치로 OS에 마운트하여 파일 시스템 생성 (iSCSI, Virtio)                  | HTTP/REST API (예: S3 API) 로 객체 접근                  |
| **주요 용도**  | 파일 공유, 홈/사용자 디렉터리, 문서 관리, 미디어 서비스 등              | 데이터베이스, VM 부팅 디스크, 트랜잭션 워크로드 등                              | 백업/아카이브, 정적 콘텐츠(미디어), 로그, 빅데이터 등                   |
| **장점**     | 사용이 쉽고 익숙함, 여러 노드 동시 접근 가능                       | 낮은 지연/높은 성능, 파일 시스템 지원, 세밀한 데이터 수정 가능                       | 무제한 확장성, 풍부한 메타데이터 관리, 비용 효율적                      |
| **단점**     | 수평 확장 필요(Scale-out), 메타데이터/잠금 관리 복잡              | 구축 비용 높음, 메타데이터 단순, 특정 호스트 의존적                              | 객체 전체를 다시 써야 함, 낮은 처리량, 일관성/수정 어려움                 |
| **서비스 예**  | AWS EFS, Azure Files, GCP Filestore, Rook-CephFS | AWS EBS, Azure Managed Disks, GCP Persistent Disk, Ceph RBD | AWS S3, Azure Blob, GCP Storage, Rook-Ceph RadosGW |

